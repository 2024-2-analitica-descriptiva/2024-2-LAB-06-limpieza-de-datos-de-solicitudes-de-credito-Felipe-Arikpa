{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este libro muestra una mejor forma de hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.options.display.max_rows = None\n",
    "\n",
    "sol_cred = pd.read_csv('../files/input/solicitudes_de_credito.csv', sep=';', index_col=0)\n",
    "sol_cred.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_strings(dataset, campo):\n",
    "\n",
    "    import re\n",
    "    dataset.copy()\n",
    "\n",
    "    dataset[campo] = dataset[campo].str.lower()\n",
    "    dataset[campo] = dataset[campo].str.replace('_' , ' ')\n",
    "    dataset[campo] = dataset[campo].str.replace('-' , ' ')\n",
    "    dataset[campo] = dataset[campo].str.replace('.' , '')\n",
    "    dataset[campo] = dataset[campo].str.replace('soli diaria' , 'solidaria')\n",
    "    dataset[campo] = dataset[campo].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x).replace(\"\\t\", \" \")) if pd.notnull(x) else x)\n",
    "    dataset[campo] = dataset[campo].str.strip()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_frase(frase):\n",
    "\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except (ImportError, LookupError):\n",
    "        !pip install nltk\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "    tokens = word_tokenize(frase)\n",
    "    frase_limpia = \" \".join([word for word in tokens if word not in spanish_stopwords])\n",
    "\n",
    "    return frase_limpia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_fechas(dataset, campo):\n",
    "\n",
    "    dataset.copy()\n",
    "\n",
    "    dataset['fecha_homologada'] = pd.to_datetime(dataset[campo], dayfirst=True, errors='coerce')\n",
    "    no_identificadas = dataset['fecha_homologada'].isnull()\n",
    "    dataset.loc[no_identificadas, 'fecha_homologada'] = pd.to_datetime(dataset.loc[no_identificadas, campo], format=\"%Y/%m/%d\", errors='coerce')\n",
    "\n",
    "    dataset.drop(columns=campo, inplace=True)\n",
    "    dataset.rename(columns={'fecha_homologada':'fecha_de_beneficio'}, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_numeros(valor):\n",
    "\n",
    "    import re\n",
    "\n",
    "    valor = valor.replace('$' , '')\n",
    "    valor = valor.replace(',' , '')\n",
    "    valor = valor.replace('.00' , '')\n",
    "    valor = re.sub(r\"\\s+\", \"\", valor)\n",
    "    valor = valor.strip()\n",
    "\n",
    "    return int(float(valor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_cred = limpieza_strings(dataset=sol_cred, campo='sexo')\n",
    "sol_cred = limpieza_strings(dataset=sol_cred, campo='tipo_de_emprendimiento')\n",
    "sol_cred = limpieza_strings(dataset=sol_cred, campo='idea_negocio')\n",
    "sol_cred = limpieza_strings(dataset=sol_cred, campo='barrio')\n",
    "sol_cred = limpieza_strings(dataset=sol_cred, campo='l√≠nea_credito')\n",
    "sol_cred['idea_negocio'] = sol_cred['idea_negocio'].apply(tokenizar_frase)\n",
    "sol_cred['barrio'] = sol_cred['barrio'].apply(tokenizar_frase)\n",
    "sol_cred = limpieza_fechas(dataset=sol_cred, campo='fecha_de_beneficio')\n",
    "sol_cred['monto_del_credito'] = sol_cred['monto_del_credito'].apply(lambda x: limpieza_numeros(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_cred.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_directory = os.path.join('files', 'output')\n",
    "output_file = os.path.join(output_directory, 'solicitudes_de_credito.csv')\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "sol_cred.to_csv(output_file, index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
